{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a729ed6d",
   "metadata": {},
   "source": [
    "# GAN-Based Image Generation\n",
    "\n",
    "This notebook implements a Generative Adversarial Network (GAN) for generating synthetic images based on training data.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Build a Generator network to create images from noise\n",
    "2. Build a Discriminator network to distinguish real vs fake images\n",
    "3. Train both networks adversarially\n",
    "4. Generate new images based on learned patterns\n",
    "5. Visualize training progress and results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912f0b1",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb3d0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU Available: False\n",
      "Keras version: 3.12.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Progress bar\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6665b",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "IMG_CHANNELS = 3\n",
    "LATENT_DIM = 100  # Dimension of noise vector\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA_1 = 0.5  # Adam optimizer parameter\n",
    "\n",
    "# Directories\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Image size: {IMG_HEIGHT}x{IMG_WIDTH}x{IMG_CHANNELS}\")\n",
    "print(f\"  Latent dimension: {LATENT_DIM}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6ebb9",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing\n",
    "\n",
    "Load your training images from the data directory. You can use any dataset of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a773d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(data_path, img_height=64, img_width=64):\n",
    "    \"\"\"\n",
    "    Load images from directory and preprocess them.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to directory containing images\n",
    "        img_height: Target image height\n",
    "        img_width: Target image width\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed images as numpy array\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    \n",
    "    image_files = [f for f in os.listdir(data_path) \n",
    "                   if Path(f).suffix.lower() in valid_extensions]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(\"‚ö†Ô∏è  No images found in data directory!\")\n",
    "        print(\"   Using synthetic data for demonstration...\")\n",
    "        # Generate synthetic data for demo\n",
    "        return generate_synthetic_data(1000, img_height, img_width)\n",
    "    \n",
    "    print(f\"Loading {len(image_files)} images...\")\n",
    "    \n",
    "    for img_file in tqdm(image_files):\n",
    "        try:\n",
    "            img_path = os.path.join(data_path, img_file)\n",
    "            img = load_img(img_path, target_size=(img_height, img_width))\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_file}: {e}\")\n",
    "    \n",
    "    if not images:\n",
    "        return generate_synthetic_data(1000, img_height, img_width)\n",
    "    \n",
    "    # Convert to numpy array and normalize to [-1, 1]\n",
    "    images = np.array(images)\n",
    "    images = (images - 127.5) / 127.5\n",
    "    \n",
    "    return images\n",
    "\n",
    "def generate_synthetic_data(num_samples, height, width):\n",
    "    \"\"\"\n",
    "    Generate synthetic training data for demonstration.\n",
    "    Creates simple geometric patterns.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_samples} synthetic images...\")\n",
    "    images = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create image with random colors and simple shapes\n",
    "        img = np.random.rand(height, width, 3) * 0.3\n",
    "        \n",
    "        # Add random circles\n",
    "        num_circles = np.random.randint(2, 6)\n",
    "        for _ in range(num_circles):\n",
    "            cx, cy = np.random.randint(0, width), np.random.randint(0, height)\n",
    "            radius = np.random.randint(5, 20)\n",
    "            color = np.random.rand(3)\n",
    "            \n",
    "            y, x = np.ogrid[:height, :width]\n",
    "            mask = (x - cx)**2 + (y - cy)**2 <= radius**2\n",
    "            img[mask] = color\n",
    "        \n",
    "        images.append(img)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    # Normalize to [-1, 1]\n",
    "    images = (images - 0.5) / 0.5\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load training data\n",
    "train_images = load_and_preprocess_images(DATA_DIR, IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print(f\"\\nDataset shape: {train_images.shape}\")\n",
    "print(f\"Value range: [{train_images.min():.2f}, {train_images.max():.2f}]\")\n",
    "print(f\"Number of training images: {len(train_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31146d6",
   "metadata": {},
   "source": [
    "### Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, n_rows=2, n_cols=5, title=\"Sample Images\"):\n",
    "    \"\"\"\n",
    "    Plot a grid of images.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            # Denormalize from [-1, 1] to [0, 1]\n",
    "            img = (images[i] + 1) / 2.0\n",
    "            img = np.clip(img, 0, 1)\n",
    "            ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample training images\n",
    "sample_indices = np.random.choice(len(train_images), 10, replace=False)\n",
    "plot_images(train_images[sample_indices], title=\"Training Data Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f22c7",
   "metadata": {},
   "source": [
    "## 4. Build the Generator\n",
    "\n",
    "The Generator takes random noise and transforms it into an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997567da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, img_shape):\n",
    "    \"\"\"\n",
    "    Build the Generator model.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension of input noise vector\n",
    "        img_shape: Output image shape (height, width, channels)\n",
    "    \n",
    "    Returns:\n",
    "        Generator model\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Generator')\n",
    "    \n",
    "    # Foundation: Start with dense layer\n",
    "    n_nodes = 8 * 8 * 256\n",
    "    model.add(layers.Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    \n",
    "    # Upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Conv2D(img_shape[2], (7, 7), activation='tanh', padding='same'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display generator\n",
    "generator = build_generator(LATENT_DIM, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "generator.summary()\n",
    "\n",
    "# Test generator output\n",
    "noise = tf.random.normal([1, LATENT_DIM])\n",
    "generated_image = generator(noise, training=False)\n",
    "print(f\"\\nGenerated image shape: {generated_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3351d9de",
   "metadata": {},
   "source": [
    "## 5. Build the Discriminator\n",
    "\n",
    "The Discriminator classifies images as real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de878858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape):\n",
    "    \"\"\"\n",
    "    Build the Discriminator model.\n",
    "    \n",
    "    Args:\n",
    "        img_shape: Input image shape (height, width, channels)\n",
    "    \n",
    "    Returns:\n",
    "        Discriminator model\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='Discriminator')\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=img_shape))\n",
    "    \n",
    "    # Downsample 64x64 -> 32x32\n",
    "    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Downsample 32x32 -> 16x16\n",
    "    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Downsample 16x16 -> 8x8\n",
    "    model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Classifier\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and display discriminator\n",
    "discriminator = build_discriminator((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "discriminator.summary()\n",
    "\n",
    "# Test discriminator output\n",
    "decision = discriminator(generated_image)\n",
    "print(f\"\\nDiscriminator decision shape: {decision.shape}\")\n",
    "print(f\"Decision value (0=fake, 1=real): {decision.numpy()[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f27e0",
   "metadata": {},
   "source": [
    "## 6. Define Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    Discriminator loss: \n",
    "    - Real images should be classified as 1\n",
    "    - Fake images should be classified as 0\n",
    "    \"\"\"\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"\n",
    "    Generator loss:\n",
    "    - Fake images should fool the discriminator (classified as 1)\n",
    "    \"\"\"\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = optimizers.Adam(LEARNING_RATE, beta_1=BETA_1)\n",
    "discriminator_optimizer = optimizers.Adam(LEARNING_RATE, beta_1=BETA_1)\n",
    "\n",
    "print(\"Loss functions and optimizers configured.\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Beta_1: {BETA_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef08211",
   "metadata": {},
   "source": [
    "## 7. Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825eddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    \"\"\"\n",
    "    Single training step for both generator and discriminator.\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate fake images\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        # Discriminator predictions\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        # Calculate losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    # Apply gradients\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "print(\"Training step function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef737fa0",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4108ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, save_dir):\n",
    "    \"\"\"\n",
    "    Generate images and save them to disk.\n",
    "    \"\"\"\n",
    "    predictions = model(test_input, training=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    fig.suptitle(f'Generated Images - Epoch {epoch}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(predictions):\n",
    "            # Denormalize\n",
    "            img = (predictions[i] + 1) / 2.0\n",
    "            img = np.clip(img, 0, 1)\n",
    "            ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = save_dir / f'image_at_epoch_{epoch:04d}.png'\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "def train_gan(dataset, epochs):\n",
    "    \"\"\"\n",
    "    Train the GAN for specified number of epochs.\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(1000).batch(BATCH_SIZE)\n",
    "    \n",
    "    # Fixed seed for consistent visualization\n",
    "    seed = tf.random.normal([10, LATENT_DIM])\n",
    "    \n",
    "    # Track losses\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING GAN TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total epochs: {epochs}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Steps per epoch: {len(train_dataset)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_gen_loss = []\n",
    "        epoch_disc_loss = []\n",
    "        \n",
    "        # Training progress bar\n",
    "        pbar = tqdm(train_dataset, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for image_batch in pbar:\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            epoch_gen_loss.append(gen_loss.numpy())\n",
    "            epoch_disc_loss.append(disc_loss.numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f'{gen_loss.numpy():.4f}',\n",
    "                'D_loss': f'{disc_loss.numpy():.4f}'\n",
    "            })\n",
    "        \n",
    "        # Average losses for epoch\n",
    "        avg_gen_loss = np.mean(epoch_gen_loss)\n",
    "        avg_disc_loss = np.mean(epoch_disc_loss)\n",
    "        gen_losses.append(avg_gen_loss)\n",
    "        disc_losses.append(avg_disc_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Generator Loss: {avg_gen_loss:.4f}\")\n",
    "        print(f\"  Discriminator Loss: {avg_disc_loss:.4f}\")\n",
    "        \n",
    "        # Generate and save images every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"\\nGenerating sample images...\")\n",
    "            generate_and_save_images(generator, epoch + 1, seed, RESULTS_DIR)\n",
    "        \n",
    "        # Save model checkpoint every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint_path = MODELS_DIR / f'generator_epoch_{epoch+1}.h5'\n",
    "            generator.save(checkpoint_path)\n",
    "            print(f\"\\nüíæ Model saved: {checkpoint_path}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return gen_losses, disc_losses\n",
    "\n",
    "# Start training\n",
    "gen_losses, disc_losses = train_gan(train_images, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b9914",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gen_losses, label='Generator Loss', color='blue', linewidth=2)\n",
    "plt.plot(disc_losses, label='Discriminator Loss', color='red', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Losses Over Time', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gen_losses, label='Generator Loss', color='blue', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Generator Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "loss_plot_path = RESULTS_DIR / 'training_losses.png'\n",
    "plt.savefig(loss_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Loss plot saved: {loss_plot_path}\")\n",
    "print(f\"\\nFinal Generator Loss: {gen_losses[-1]:.4f}\")\n",
    "print(f\"Final Discriminator Loss: {disc_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94558eae",
   "metadata": {},
   "source": [
    "## 10. Generate New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, num_images=10, save=True):\n",
    "    \"\"\"\n",
    "    Generate new images using the trained generator.\n",
    "    \"\"\"\n",
    "    # Generate random noise\n",
    "    noise = tf.random.normal([num_images, LATENT_DIM])\n",
    "    \n",
    "    # Generate images\n",
    "    generated_images = generator(noise, training=False)\n",
    "    \n",
    "    # Plot results\n",
    "    n_rows = 2\n",
    "    n_cols = 5\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6))\n",
    "    fig.suptitle('Generated Images (Final Model)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(generated_images):\n",
    "            # Denormalize\n",
    "            img = (generated_images[i] + 1) / 2.0\n",
    "            img = np.clip(img, 0, 1)\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            if save:\n",
    "                # Save individual image\n",
    "                img_pil = array_to_img(img)\n",
    "                img_path = RESULTS_DIR / f'generated_image_{i+1}.png'\n",
    "                img_pil.save(img_path)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    grid_path = RESULTS_DIR / 'final_generated_images.png'\n",
    "    plt.savefig(grid_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    if save:\n",
    "        print(f\"\\n‚úÖ Generated {num_images} images\")\n",
    "        print(f\"üìÅ Saved to: {RESULTS_DIR}\")\n",
    "    \n",
    "    return generated_images\n",
    "\n",
    "# Generate final images\n",
    "print(\"\\nGenerating final images...\")\n",
    "final_images = generate_images(generator, num_images=10, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fc753",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78589427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "generator_path = MODELS_DIR / f'generator_final_{timestamp}.h5'\n",
    "discriminator_path = MODELS_DIR / f'discriminator_final_{timestamp}.h5'\n",
    "\n",
    "generator.save(generator_path)\n",
    "discriminator.save(discriminator_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ MODELS SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Generator: {generator_path}\")\n",
    "print(f\"Discriminator: {discriminator_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b88dfd",
   "metadata": {},
   "source": [
    "## 12. Load and Use Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba03ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved generator and generate images\n",
    "# Uncomment and update the path to use\n",
    "\n",
    "# loaded_generator = keras.models.load_model(generator_path)\n",
    "# print(\"Generator loaded successfully!\")\n",
    "\n",
    "# # Generate new images with loaded model\n",
    "# generate_images(loaded_generator, num_images=5, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eaed70",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ Built a complete GAN architecture (Generator + Discriminator)\n",
    "2. ‚úÖ Trained the model on image data\n",
    "3. ‚úÖ Generated synthetic images from random noise\n",
    "4. ‚úÖ Visualized training progress\n",
    "5. ‚úÖ Saved trained models for future use\n",
    "\n",
    "### To Improve Results:\n",
    "- **More Training Data**: Use larger, more diverse image datasets\n",
    "- **Longer Training**: Increase epochs (200-500+)\n",
    "- **Architecture Tweaks**: Experiment with layer sizes and depths\n",
    "- **Advanced Techniques**: Try Progressive GAN, StyleGAN, or Conditional GAN\n",
    "- **Hyperparameter Tuning**: Adjust learning rates, batch sizes, etc.\n",
    "\n",
    "### Common Issues:\n",
    "- **Mode Collapse**: Generator produces limited variety ‚Üí Reduce learning rate\n",
    "- **Training Instability**: Losses oscillate wildly ‚Üí Add gradient penalty, use label smoothing\n",
    "- **Poor Quality**: Blurry or unrealistic images ‚Üí Train longer, use more data\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with your own image datasets\n",
    "2. Try conditional GANs for controlled generation\n",
    "3. Explore image-to-image translation tasks\n",
    "4. Implement advanced GAN architectures (DCGAN, WGAN, StyleGAN)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy experimenting with GANs! üé®ü§ñ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
